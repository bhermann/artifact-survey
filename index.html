<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Artifact Evaluation Survey (Additional Material)</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  <head>

  <body>
    <header>
      <div class="jumbotron">
        <h1>Community Expectations for Research Artifacts and Evaluation Processes (Additional Material)</h1>
        <h2><a href="https://thewhitespace.de/">Ben Hermann</a>, <a href="https://www.stefan-winter.net/">Stefan Winter</a>, <a href="https://www.tu-chemnitz.de/informatik/ST/professur/professor.php">Janet Siegmund</a></h2>
      </div>
    </header>
    <main>
      <div class="container">
        <p class="lead">This repository contains the material used in and produced during the study <em>Community Expectations for Research Artifacts and Evaluation Processes</em> accepted at the <a href="https://2020.esec-fse.org/">ACM Joint
            European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2020</a>.
          <h3>Abstract</h3>

          <p><em>Background.</em> Artifact evaluation has been introduced into the software engineering and programming languages research community with a pilot at ESEC/FSE 2011 and has since then enjoyed a healthy adoption throughout the
            conference landscape.</p>

          <p><em>Objective.</em>
            In this qualitative study, we examine the expectations of the community toward research artifacts and their evaluation processes.</p>

          <p><em>Method.</em>
            We conducted a survey including all members of artifact evaluation committees of major conferences in the software engineering and programming language field since the first pilot and compared the answers to expectations set by calls for
            artifacts and reviewing guidelines.</p>

          <p><em>Results.</em>
            While we find that some expectations exceed the ones expressed in calls and reviewing guidelines, there is no consensus on quality thresholds for artifacts in general.
            We observe very specific quality expectations for specific artifact types for review and later usage, but also a lack of their communication in calls.
            We also find problematic inconsistencies in the terminology used to express artifact evaluation's most important purpose -- <em>replicability</em>.</p>

          <p><em>Conclusion.</em>
            We derive several actionable suggestions which can help to mature artifact evaluation in the inspected community and also to aid its introduction into other communities in computer science.</p>


          <h3>Paper Preprint</h3>

          <p>The paper preprint is available here.</p>

          <h3>Artifact Availability</h3>

          <p>The artifact is maintained as a <a href="https://github.com/bhermann/artifact-survey">GitHub repository</a>.<br>
            It is <a href="https://github.com/bhermann/artifact-survey/releases">versioned according to the state of the publication</a>.<br>
            Released versions are automatically archived to Zenodo.</p>

          <h4>Version History</h4>
          <table class="table">
            <thead>
              <tr>
                <th>Version</th>
                <th>Archive (for previous versions)</th>
                <th>Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>1</td>
                <td><a href="https://zenodo.org/badge/latestdoi/203532929"><img src="https://zenodo.org/badge/203532929.svg" alt="DOI"></a></td>
                <td>Blinded version available to the paper reviewers.</td>
              </tr>
              <tr>
                <td>2</td>
                <td></td>
                <td>Version available for Artifact Evaluation</td>
              </tr>
            </tbody>
          </table>

          <h3>Contents</h3>
          <ul>
            <li>
              Calls for Artifacts
              <ul>
                <li><span class="badge badge-danger">Data</span> <span class="badge badge-info">Raw</span> <a class="" href="calls/index.html">Collected Calls for Artifact</a>(Collected Calls for Artifacts (CfA) of the inspected conferences.)</li>
                <li><span class="badge badge-danger">Data</span> <span class="badge badge-success">Derived</span> <a class="" href="results/aec.xlsx">Artifact Evaluation Commitees (collected from CfAs) (Excel)</a> - e-mail addresses removed</li>
                <li><span class="badge badge-danger">Data</span> <span class="badge badge-success">Derived</span> <a href="analysis/calls/analysis.csv">Card sorting results from calls off artifacts</a> </li>
                <li><span class="badge badge-warning">R Script</span> <a href="analysis/calls/analysis.R">Analysis script for CfA tags</a> </li>
              </ul>
            </li>
            <li>
              Survey
              <ul>
                <li><a class="" href="questionnaire/index.html">Survey</a> (Our survey questions in printable form. Survey was filled out online.)</li>
                <li><span class="badge badge-danger">Data</span> <span class="badge badge-info">Raw</span> <a href="results/results-survey54231.xlsx">Raw result data file (Excel)</a> - anonymized at F3</li>
                <li><span class="badge badge-danger">Data</span> <span class="badge badge-success">Derived</span> <a class="" href="results/cardsorting.html">Card sorting results</a> (Labels obtained with the open card sorting method.)</li>
                <li>
                  Plots
                  <ul>
                    <li><span class="badge badge-warning">R Script</span> <a href="analysis/survey/conferencespread.R">Committee sizes and responses</a> (Figure 1)</li>
                    <li><span class="badge badge-warning">R Script</span> <a href="analysis/survey/participant_stats.R">Histogram of individuals by number of AECs served in</a> (Figure 2)</li>
                    <li><span class="badge badge-primary">Output</span> <a class="" href="figures/index.html">Figures</a> (Figures derived from the collected data.)</li>
                  </ul>
                </li>
                <li>
                  Analyses
                  <ul>
                    <li><span class="badge badge-warning">R Script</span> <a href="analysis/survey/numericdata.R">Analysis of questions with numeric answers</a></li>
                    <li><span class="badge badge-primary">Output</span> <a href="results/numericresults.txt">Results from the analysis of numeric answers</a> </li>
                    <li><span class="badge badge-warning">R Script</span> <a href="analysis/survey/taganalysis.R">Analysis of full-text answers using the tags from open card sorting</a></li>
                    <li><span class="badge badge-primary">Output</span> <a href="results/tagresults.txt">Results from the analysis of open card sorting tags</a> </li>
                  </ul>
                </li>
                <li><span class="badge badge-warning">R Script</span> <a href="analysis/survey/runall.R">Script to run both plots and both analysis scripts</a> </li>
              </ul>
            </li>
          </ul>

          <h4>Legend</h4>
          <dl class="row">
            <dt class="col-sm-1"><span class="badge badge-danger">Data</span></dt>
            <dd class="col-sm-11">Data artifact</dd>
            <dt class="col-sm-1"><span class="badge badge-info">Raw</span></dt>
            <dd class="col-sm-11">Raw data collected</dd>
            <dt class="col-sm-1"><span class="badge badge-success">Derived</span></dt>
            <dd class="col-sm-11">Data derived from the collected data</dd>
            <dt class="col-sm-1"><span class="badge badge-warning">R Script</span></dt>
            <dd class="col-sm-11">Analysis script written in R</dd>
            <dt class="col-sm-1"><span class="badge badge-primary">Output</span></dt>
            <dd class="col-sm-11">Script output</dd>
          </dl>

          <h3>Requirements</h3>
          <p>
            The analysis script provided in this artifact require <a href="https://www.r-project.org/">R</a> to run.
            We were using <a href="https://cran.r-project.org/src/base/R-3/">R version 3.6.1 (2019-07-05)</a> at the time of the study.
            The scripts make use of the following packages and were executed with the following versions of these packages:
          </p>
          <table class="table table-sm">
            <thead>
              <tr>
                <th>Package name</th>
                <th>Version used</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>dplyr</td>
                <td>0.8.3</td>
              </tr>
              <tr>
                <td>ggplot2</td>
                <td>3.2.0</td>
              </tr>
              <tr>
                <td>readxl</td>
                <td>1.3.1</td>
              </tr>
              <tr>
                <td>stringr</td>
                <td>1.4.0</td>
              </tr>
              <tr>
                <td>svglite</td>
                <td>1.2.3</td>
              </tr>
              <tr>
                <td>tibble</td>
                <td>2.1.3</td>
              </tr>
              <tr>
                <td>tidyr</td>
                <td>0.8.3 </td>
              </tr>
            </tbody>
          </table>

          <p>
            All scripts will automatically install the necessary packages if not present in the current environment.
            However, there were cases in the past when functions were deprecated by the package authors.
            During the time of the study we did not experience such a case, but they might appear in the future.
          </p>

          <p>Data files are provided as CSV or Excel files. They can be read and processed with R or other capable environments (e.g. pandas, Excel itself, ...)</p>

          <h3>Executing scripts</h3>

          <p>
            To create the scripts and inspect results, we used <a href="https://rstudio.com/">RStudio</a>.
            Thus, the scripts may either be executed in RStudio or directly on the command-line.
            For the latter, please use the following format:
            <code>
R < scriptname.R --vanilla
</code> to avoid interference with other projects. The <code>analysis/survey</code> folder contains a convenient <code>runAll.R</code> script to run all scripts for the analysis of survey results. Their output will be placed in the <code>analysis/survey/output</code> folder. Warnings may occur, but were checked during development. 
          </p>
      </div>
    </main>
    <footer>
      <div class="container">
        <p class="bg-warning">
          Version 2 - Pre-Artifact Evaluation
        </p>
      </div>
    </footer>
  </body>

</html>
