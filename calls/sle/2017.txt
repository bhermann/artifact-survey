Artifact Evaluation
SLE will for the second year use an evaluation process for assessing the quality of artifacts on which papers are based. The aim of this evaluation process is to foster a culture of experimental reproducibility as well as a higher quality in the research area as a whole.

Authors of papers accepted for SLE 2017 will be invited to submit artifacts. Any kind of artifact that is presented in the paper, supplements the paper with further details, or underlies the paper can be submitted. This includes, for instance, tools, grammars, metamodels, models, programs, algorithms, scripts, proofs, datasets, statistical tests, checklists, surveys, interview scripts, visualizations, annotated bibliographies, and tutorials.

The submitted artifacts will be reviewed by a dedicated Artifact Evaluation Committee (AEC). Artifacts that live up to the expectations created by the paper will receive a badge of approval from the AEC. The approved artifacts will be invited for inclusion in the electronic conference proceedings published in the ACM Digital Library. This will ensure the permanent and durable storage of the artifacts alongside the published papers fostering the repeatability of experiments, enabling precise comparison with alternative approaches, and helping the dissemination of the author’s ideas in detail.

The AEC will award the artifact that most significantly exceeds the expectations with a Distinguished Artifact Award.

Participating in the artifact evaluation and publishing approved artifacts in the ACM Digital Library is voluntary. However, we strongly encourage authors to consider this possibility as the availability of artifacts will greatly benefit readers of papers and increase the impact of the work. Note that the artifact evaluation cannot affect the acceptance of the paper, because it only happens after the decision about acceptance has been made.

The artifact evaluation process of SLE borrows heavily from processes described at artifact-eval.org, ECOOP 2016 and ICSME 2016. The applied process is detailed in the following.

Submission
If and when your paper has been accepted for SLE 2017, you will be invited by the AEC chairs to submit the artifacts that underlie your work. This invitation will contain detailed instructions on how to submit your artifacts.

An artifact submission comprises the following components:

Paper: Preliminary PDF version of the accepted SLE 2017 paper. The paper will be used to evaluate the consistency of the accepted paper and the submitted artifact, as well as to assess whether the artifact lives up to the expectations created by the paper.
Authors of the artifact: This list may include people who are not authors of the accepted paper, but contributed to creating the artifact.
Abstract: A short description of the artifact to be used for assignments of artifacts to AEC members.
Artifact: An archive file (gz, xz, or zip) containing everything needed for supporting a full evaluation of the artifact. The archive file has to include at least the artifact itself and a text file “README.txt” that contains the following information:
An overview of the archive file documenting the content of the archive.
A setup / installation guide giving detailed instructions on how to setup or install the submitted artifact.
Detailed step-by-step instructions on how to reproduce any experiments or other activities that support the conclusions given in the paper.
If multiple artifacts underlie an accepted SLE paper, all artifacts should be collected in one archive and submitted together in one single submission. For instance, if a tool has been developed, a tutorial has been authored with detailed instructions on how to use the tool, and user studies have been performed for evaluating the tool’s properties, the tool, the tutorial, and the raw data collected in the user study should be packed in one archive file and submitted together in one single submission to the SLE 2017 artifact evaluation.

When preparing your artifact, consider that your artifact should be as accessible to the AEC as possible. In particular, it should be possible for the AEC to quickly make progress in the investigation of your artifact. Please provide some simple scenarios describing concretely how the artifact is intended to be used. For a tool, this would include specific inputs to provide or actions to take, and expected output or behavior in response to this input.

For artifacts that are tools, it is recommended to provide the tool installed and ready to use on a virtual machine for VirtualBox, VMware, SHARE or a similar widely available platform.

Please use widely supported open formats for documents (e.g., PDF, HTML) and data (e.g., CSV, JSON).

Evaluation Process
Submitted artifacts will be evaluated by the AEC concerning the following criteria: Artifacts should be

consistent with the paper,
as complete as possible,
well documented, and
easy to (re)use facilitating further research.
Each submitted artifact will be evaluated by at least two members of the AEC. Thereby, the artifacts will be treated confidentially, just like the submitted paper.

Artifacts that pass the evaluation will receive an “Artifact Evaluated - Functional” badge and be invited for inclusion in the electronic conference proceedings published in the ACM Digital Library. Artifacts that will be included in the ACM Digital Library or that will be made permanently available in another publicly accessible archival repository will also receive the “Artifact Available” badge. Detailed definitions of these badges and the respective evaluation criteria may be found at the ACM Artifact Review Badging site.

The evaluation consists of two steps:

Kicking-the-tires: Reviewers will check the artifact’s integrity and look for any possible setup problems that may prevent it from being properly evaluated (e.g., corrupted or missing files, VM won’t start, immediate crashes on the simplest example, etc.). In case of any problems, authors will be given a 48-hour period (August 17-18) to read and respond to the kick-the-tires reports of their artifacts and solve any issues preventing the artifact evaluation.
Artifact assessment: Reviewers evaluate the artifacts and decide on the approval of the artifact.
Notification about the outcome of the artifact evaluation and reviews including suggestions for improving the artifacts will be distributed about one week before the deadline for the final version of the research paper, such that the outcome can be mentioned in the paper and the final artifact can be uploaded for inclusion in the ACM Digital Library.

Important Dates
August 10, 2017: Artifact submission
August 17-18, 2017: Kick-the-tires author response
September 1, 2017: Artifact notification
Artifact Evaluation Chairs
Tanja Mayerhofer, TU Wien, Austria
Laurence Tratt, King’s College London, UK
Further Information
For further information on the artifact evaluation of SLE 2017, feel free to contact the artifact evaluation chairs with an e-mail to sle2017ae@googlegroups.com.

Chairs
Tanja Mayerhofer, TU Wien, Austria
Laurence Tratt, King’s College London, UK

Committee
Edouard Batot, University of Montreal, Canada
Erwan Bousse, TU Wien, Austria
Thomas Degueule, CWI, The Netherlands
Lukas Diekmann, King’s College London, UK
Romina Eramo, University of L’Aquila, Italy
Abel Gómez, Open University Catalonia, Spain
Thomas Kühn, TU Dresden, Germany
Manuel Leduc, IRISA, University of Rennes 1, France
Julian Mackay, Victoria University of Wellington, New Zealand
Matthew Roberts, Macquarie University, Australia
Vasco Sousa, University of Montreal, Canada
Daniel Strüber, University of Koblenz-Landau, Germany
Matúš Sulír, Technical University of Košice, Slovakia
Ana Sutii, Eindhoven University of Technology, The Netherlands
Andreas Wortmann, RWTH Aachen, Germany
